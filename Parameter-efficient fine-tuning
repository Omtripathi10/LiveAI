# Install required libraries
!pip install peft transformers accelerate bitsandbytes -q

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Load Model & Tokenizer (using 8-bit for memory efficiency)
model_id = "google/gemma-2b-it"  # Use smaller, optimized model
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,   # Reduce memory usage
    load_in_8bit=True,           # Enable 8-bit quantization (saves RAM)
    device_map="auto"
)

# ✅ Apply LoRA for Efficient Finetuning
def apply_lora(model):
    # Define LoRA config
    lora_config = LoraConfig(
        r=8,  # LoRA rank (tradeoff between performance & efficiency)
        lora_alpha=16,  # Scaling factor
        lora_dropout=0.05,  # Dropout for regularization
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"
        ],
    )

    # Apply LoRA to the model
    lora_model = get_peft_model(model, lora_config)
    return lora_model

model = apply_lora(model)

# ✅ Print Trainable Parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable Parameters: {trainable_params}")
print(f"Total Parameters: {total_params}")
print(f"Percentage of Trainable Params: {trainable_params / total_params * 100:.2f}%")
